> [!NOTE]
> When converting from SQL to PySpark, the order of operations display is often reversed.


### Counting Null Values

SQL:
```sql
SELECT COUNT_IF(column1 IS NULL)
-- OR 
SELECT COUNT() FROM TABLE_1 WHERE column1 IS NULL
```

PySpark:
```python
table = spark.read.table(...)

table.selectExpr("SELECT COUNT_IF(column1 IS NULL) FROM TABLE_1")
# OR
table.where(col("column1").isNull()).count()
```

---

### Count Distinct Values

SQL:
```sql
SELECT
	COUNT(DISTINCT)
FROM 
	TABLE_1
```

PySpark:
```python
table = spark.read.table(...)

table.distinct().count()
```

---

### Deduplication

SQL:
```sql
SELECT
	key_column1,
	key_column2,
	...
	MAX(column1) 
FROM 
	TABLE_1 
GROUP BY 
	key_column1,
	key_column2
```

PySpark:
```python
new_Table = (
    table
    .where(col("user_id").isNotNull())
    .groupBy("key_column")
    .agg(max("column1").alias("column1_alias"))
)
```


---

### Counting Duplicates

SQL:
```sql
SELECT 
	user_id,
	MAX(row_count) > 1 AS duplicates 
FROM (
    SELECT 
	    user_id, 
	    COUNT(*) AS row_count 
    FROM 
	    TABLE_1
    GROUP BY 
		user_id
)
```

PySpark:
```python
# Load the table into a DataFrame
df = spark.read.table("TABLE_1")

# Group by user_id and count the rows
grouped_df = df.groupBy("user_id").agg(count("*").alias("row_count"))

# Check if the maximum row count is greater than 1
result_df = grouped_df.agg((max_("row_count") > 1).alias("duplicates"))
```

---

### Date Formatting

SQL:
```sql
SELECT 
	DATE_FORMAT(column, 'mmDDyy')
FROM
	TABLE_1
```

PySpark:
```python
df = spark.read.table("TABLE_1")

df = (
	  df.withColumn("new_column", date_format("column",'mmDDyy').cast("timestamp"))
)
```

---

### JSON Array Operations


 - Check JSON array size:
  ```sql
  SELECT * FROM table WHERE SIZE(column) > 1
  ```
- `collect_set`: Collects unique values for a field
- `flatten`: Combines arrays into a single array
- `array_distinct`: Removes distinct values

![[Pasted image 20240821170521.png]]

![[Pasted image 20240821170509.png]]

> [!TIP]
> Use `column.whatever` to navigate down the array tree.

---

### SQL UDFs

SQL UDFs allow you to register custom SQL logic as functions in the database, making these methods reusable anywhere SQL can be run on Databricks. They are registered locally and take advantage of Spark optimizations when executed.

SQL:
```sql
CREATE OR REPLACE FUNCTION sales_announcement(item_name, item_price)
RETURNS STRING
RETURN CONCAT('The ', item_name, ' is on sale for ', item_price);

SELECT 
	*, 
	sales_announcement(name, price) AS message 
FROM 
	item_lookup
```

- Use `DESCRIBE FUNCTION` to view the SQL logic in the function body.
- Useful for complex `CASE WHEN` statements.

---

### Python UDFs

Python UDFs are custom column transformation functions with some limitations:
- Cannot be optimized by Spark
- Function is serialized and sent to executors
- Requires serialization and deserialization to use the UDF (performance impact)
- Extra communication between the driver and executors

> [!WARNING]
> Python UDFs can have significant performance overhead due to serialization/deserialization and additional communication.


---

### Generated Columns

Generated columns are a special type of column whose values are automatically generated based on the values of other columns.

SQL:
```sql
CREATE TABLE default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  dateOfBirth DATE GENERATED ALWAYS AS (CAST(birthDate AS DATE)),
  ssn STRING,
  salary INT
)
```


PySpark:
```python
table.create(spark) \
  .tableName("default.people10m") \
  .addColumn("id", "INT") \
  .addColumn("firstName", "STRING") \
  .addColumn("middleName", "STRING") \
  .addColumn("lastName", "STRING", comment = "surname") \
  .addColumn("gender", "STRING") \
  .addColumn("birthDate", "TIMESTAMP") \
  .addColumn("dateOfBirth", DateType(), generatedAlwaysAs="CAST(birthDate AS DATE)") \
  .addColumn("ssn", "STRING") \
  .addColumn("salary", "INT") \
  .execute()

```

---

### Merge

MERGE can combine two tables with configurations for update, insert, or delete operations.

![[Pasted image 20240828113847.png]]
![[Pasted image 20240828113855.png]]

> [!WARNING]
>  Can have problems if a column is generated and the values do not match.

---
# Window Functions

Window functions  allow you to perform calculations across a set of rows that are related to the current row.
## Basic Syntax

The general syntax for a window function in Spark SQL is:

```sql
window_function(args) OVER (
    [PARTITION BY partition_expression, ...]
    [ORDER BY sort_expression [ASC | DESC], ...]
)
```

- `PARTITION BY`: Divides the rows into partitions to which the window function is applied.
- `ORDER BY`: Specifies the ordering of rows within each partition.

## Types of Window Functions

### 1. Ranking Functions

#### ROW_NUMBER()

Assigns a unique integer to each row within a partition.

Example:
```sql
SELECT 
  employee_id, 
  department, 
  salary,
  ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank
FROM employees
```

#### RANK() and DENSE_RANK()

Both assign a rank to each row within a partition, but handle ties differently:
- `RANK()` leaves gaps in the ranking for ties.
- `DENSE_RANK()` doesn't leave gaps.

Example:
```sql
SELECT 
  employee_id, 
  department, 
  salary,
  RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank,
  DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank
FROM employees
```

### 2. Offset Functions

#### LAG() and LEAD()

Access data from previous or subsequent rows in relation to the current row.

Example:
```sql
SELECT 
  date, 
  sales,
  LAG(sales) OVER (ORDER BY date) as previous_day_sales,
  LEAD(sales) OVER (ORDER BY date) as next_day_sales
FROM daily_sales
```

### 3. Aggregate Window Functions

Apply aggregate functions over a window of rows.

#### Running Total

Example:
```sql
SELECT 
  date, 
  sales,
  SUM(sales) OVER (ORDER BY date) as running_total
FROM daily_sales
```

#### Moving Average

Example:
```sql
SELECT 
  date, 
  temperature,
  AVG(temperature) OVER (
    ORDER BY date 
    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
  ) as moving_avg_3day
FROM weather_data
```

### 4. Percentile Functions

#### NTILE()

Divides the rows into a specified number of ranked groups.

Example:
```sql
SELECT 
  employee_id, 
  salary,
  NTILE(4) OVER (ORDER BY salary) as salary_quartile
FROM employees
```

## Advanced Concepts

### Window Frame Specification

The frame clause defines the set of rows within the partition for the function to operate on.

Syntax:
```sql
{ROWS | RANGE} BETWEEN frame_start AND frame_end
```

Example (Cumulative sum within a 7-day window):
```sql
SELECT 
  date, 
  sales,
  SUM(sales) OVER (
    ORDER BY date 
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) as rolling_7day_sum
FROM daily_sales
```

### Multiple Window Functions

You can use multiple window functions in a single query:

```sql
SELECT 
  employee_id, 
  department, 
  salary,
  AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,
  salary - AVG(salary) OVER (PARTITION BY department) as salary_diff_from_avg,
  RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_salary_rank
FROM employees
```

## Best Practices

1. Use appropriate partitioning to limit the size of the window when dealing with large datasets.
2. Be mindful of the ordering within partitions, as it can significantly affect results.
3. Consider performance implications when using complex window functions on very large datasets.