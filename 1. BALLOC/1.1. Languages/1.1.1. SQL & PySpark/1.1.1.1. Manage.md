
### SELECT _ FROM *file_format*

SQL:
```sql
SELECT 
	column1,
	column2
	...
FROM 
	[FILE_FORMAT].[FILE_PATH]
```

> [!NOTE]
> Works with the following formats (possibily even more):
> - XML
> - CSV
> - PARQUET
> - YAML
> - AVRO

---

### SELECT _ FROM *directory*

SQL:
```sql
SELECT 
	column1,
	column2
	...
FROM 
	[FILE_FORMAT].[DIRECTORY_PATH]
```

> [!NOTE]
> Works with the following formats (possibily even more):
> - XML
> - CSV
> - PARQUET
> - YAML
> - AVRO

---

### CREATE TABLE

SQL:
```sql
CREATE TABLE IF NOT EXISTS table_name
(
  column1 datatype1,
  column2 datatype2,
  ...
)
USING [FILE_FORMAT]
OPTIONS (
  header = "true",
  delimiter = ","
)
LOCATION 'path/to/files'
```

PySpark:
```python
schema = StructType([
    StructField("column1", StringType(), True),
    StructField("column2", IntegerType(), True),
    # ...
])

# Create an empty DataFrame with the defined schema
df = spark.createDataFrame([], schema)

# Write the DataFrame to the specified location with the given options
df.write.format("[FILE_FORMAT]") \
    .option("header", "true") \
    .option("delimiter", ",") \
    .mode("overwrite") \
    .save("path/to/files")
```

> [!NOTE]
> Works with the following formats (possibily even more):
> - XML
> - CSV
> - PARQUET
> - YAML
> - AVRO
> - DELTA

---

### CTAs (Create Table As)

   - Limiting because you can't be very specific about the table configurations.

SQL:
```sql
CREATE TABLE table_2 AS
SELECT 
	column_1,
	column_2,
	...
FROM
	table_1
```

---

### VIEWS

- Temporary views are __only__ visible in the current Spark session. (If being run on Spark)
- Permanent views are stored in the metastore and available across sessions.

SQL:
```sql
CREATE OR REPLACE VIEW view_name AS
SELECT 
	column1, 
	column2,
	...
FROM 
	table_name
WHERE 
	condition
```

PySpark:
```python
# Load the table into a DataFrame
table_df = spark.read.table("table_name")

# Define the condition
condition = col("column_name") == "some_value"

# Select the required columns and apply the condition
view_df = table_df.select("column1", "column2", "...").where(condition)

# Create or replace the view
view_df.createOrReplaceTempView("view_name")
```

---

### CTEs (Common Table Expressions)

- Useful for breaking down complex queries into manageable parts

SQL:
```sql
WITH cte_name AS (
  SELECT 
	  column1, 
	  column2,
	  ...
  FROM 
	  table_name
  WHERE 
	  condition
)

SELECT 
	* 
FROM 
	cte_name
```

---

### Querying from JSON Columns

SQL:
```sql
SELECT
	* 
FROM 
	table_1 
WHERE 
	column:column_2 = 'oh'
```

---

### Binary Files

SQL:
```sql
SELECT 
	* 
FROM 
	binaryFile.'file/directory'`
```

Provides metadata like:
- Last update date
- Content
- Length
- Path
- File name

---

### Cache Table

> [!NOTE]
> Spark automatically caches underlying data for optimal performance in subsequent queries.


SQL:
```sql
CACHE TABLE table_2 AS
SELECT 
	*
FROM 
	table_1
```


- `REFRESH TABLE` statement invalidates the cached entries, which include data and metadata of the given table or view

```sql
REFRESH TABLE table_2
```
- Useful for execution time testing as it invalidates the cache.

---
### Constraints

SQL: 
```sql
CREATE TABLE _table_name_ (  
    column1 datatype_ _constraint_,  
    column2 datatype_ _constraint_,  
    column3 datatype_ _constraint_,  
    ....  
);
```

You can add constraints for columns using `ADD CONSTRAINT`.

SQL:
```sql
ALTER TABLE Persons  
ADD CONSTRAINT PK_Person PRIMARY KEY (ID,LastName);
```

---
### Cloning

- `DEEP CLONE`: Fully copies data and metadata.
- `SHALLOW CLONE`: Only copies Delta Transaction logs.

---

### DELTA TABLES

- `DESCRIBE HISTORY`: View Delta table history
- `COPY INTO`: Efficient for adding small files multiple times to a table
- `OPTIMIZE`: Compact small files
- `ZORDER`: Index by a specific column
- `VACUUM`: Clean files from older versions of a Delta table (use `RETAIN X HOURS`)
- `RESTORE TABLE`: Restore the table to a previous version

---
