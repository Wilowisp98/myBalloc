## PROBLEM REDUCTION

### Definition
In computational complexity theory, a problem A is reducible to problem B if an algorithm for solving problem B can be used as a subroutine to solve problem A. When this is true, solving A cannot be harder than solving B.

### Key Concept
- Reduction allows complex problems to be simplified and solved using solutions to simpler problems.
- It helps in understanding the relative difficulty of problems.

### Example: Slowly Changing Dimension (SCD) Type 2

#### Initial Problem: 6 Instances
1. Row unchanged, already inactive
2. Row unchanged, already active
3. Row changed, new row creation needed
4. Row changed, old row needs to become inactive
5. New row to be added
6. Row to be made inactive (no longer in source)

#### First Reduction: 3 Instances
1. Existing elements requiring changes
2. Completely new elements
3. Elements no longer existing

#### Final Reduction: 2 Instances
1. Column modification required
2. Column creation required

### Importance of Reduction
- Simplifies complex problems
- Allows reuse of existing solutions
- Helps in problem classification and understanding

### Application in Data Warehousing
In the SCD Type 2 example, reducing the problem to two core operations (modify and create) simplifies implementation and maintenance of the dimension table.

### Broader Applications
- Algorithm design
- Complexity analysis
- Proof techniques in theoretical computer science

### Key Takeaway
Solving the lowest (most reduced) problem often leads to solutions for the higher-level, more complex problems.

### Further Reading
1. [Reduction (complexity) - Wikipedia](https://en.wikipedia.org/wiki/Reduction_(complexity))
2. [Introduction to the Theory of Computation](https://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X)

---
## CAP Theorem in Distributed Systems

## Overview

The CAP Theorem, also known as Brewer's Theorem, is a fundamental principle in distributed computing. It states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees:

1. **C**onsistency
2. **A**vailability
3. **P**artition Tolerance

> [!important] 
> In practice, partition tolerance is necessary for distributed systems, so designers usually have to choose between consistency and availability.

## The Three Guarantees

### Consistency (C)

- All nodes see the same data at the same time.
- A read operation will return the most recent write from any node in the system.
- Ensures that every read receives the most recent write or an error.

### Availability (A)

- Every request receives a response, without guarantee that it contains the most recent version of the information.
- The system remains operational 100% of the time.
- Every node (non-failing) always returns a response.

### Partition Tolerance (P)

- The system continues to operate despite arbitrary partitioning due to network failures.
- Cluster continues to function even if there is a "partition" (communication break) between nodes.

## Visual Representation

![[Pasted image 20240830114959.png]]

## Implications in Distributed Systems

1. **CA Systems**: Sacrifice partition tolerance
   - Example: Traditional RDBMSs like PostgreSQL
   - Suitable for single-datacenter operations

2. **CP Systems**: Sacrifice availability
   - Example: MongoDB (in certain configurations), HBase
   - Preferred when consistency is critical (e.g., financial systems)

3. **AP Systems**: Sacrifice consistency
   - Example: Apache Cassandra, Amazon DynamoDB
   - Suitable for systems requiring high availability (e.g., content delivery networks)

## Considerations in System Design

When designing distributed systems, consider:

1. **Nature of the Application**: What does your application prioritize?
2. **Consistency Models**: If not fully consistent, what level of consistency is acceptable?
3. **Partition Handling**: How will the system behave during network partitions?
4. **Recovery Mechanisms**: How will the system recover and reconcile data after a partition is resolved?
## Relationship to Big Data and Spark

In big data ecosystems:
- Spark itself is not a distributed storage system, so CAP doesn't directly apply
- However, Spark often works with distributed storage systems (e.g., HDFS, Cassandra) where CAP considerations are crucial
- Understanding CAP helps in choosing the right data store for Spark applications

---

