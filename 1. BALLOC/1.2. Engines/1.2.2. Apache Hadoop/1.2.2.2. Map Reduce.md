## Overview

MapReduce is a programming model and processing technique designed for distributed computing on large datasets. It's a core component of the Hadoop ecosystem, enabling efficient parallel processing across clusters of computers.

> [!quote] 
> "MapReduce works by breaking the processing into two phases: Map and Reduce."

## Key Phases

### 1. Map Phase

- Performs the same computation on all data simultaneously
- Data is distributed and processed across different clusters
- Input and output are in key-value pairs

### 2. Shuffle Phase (Intermediate)

- Occurs between Map and Reduce phases
- Data is sorted and grouped by keys
- Prepares data for efficient reduction

### 3. Reduce Phase

- Applies reduction operations on grouped data
- Input and output are in key-value pairs
- Processes only relevant keys prepared by the Shuffle phase

## Visual Representation

![[Pasted image 20240830111236.png]]

> [!note]
> This diagram illustrates the flow of data through the Map, Shuffle, and Reduce phases.

## Data Flow in MapReduce

1. **Input**: Data is split into chunks and distributed across the cluster
2. **Map**: Each chunk is processed independently, generating key-value pairs
3. **Shuffle**: Key-value pairs are sorted and grouped by key
4. **Reduce**: Grouped data is processed to produce final output

## Key Concepts

### Data Locality

- MapReduce moves computation to data, not vice versa
- More efficient than computing data from various locations
- Reduces network congestion and improves processing speed

### Fault Tolerance

- If any Map task fails, the Reduce phase is delayed
- System waits for the failed task to be recovered and re-executed
- Ensures data integrity and complete processing

### Integration with HDFS and YARN

- Hadoop moves MapReduce tasks to machines hosting data partitions
- YARN (Yet Another Resource Negotiator) manages resource allocation
- Combines distributed storage (HDFS) with distributed processing (MapReduce)

## Storage Considerations

### Map Output Storage

- Map phase results are written to local disk, not HDFS
- Rationale: Map results are temporary and disposable after computation
- Writing to HDFS would be inefficient for intermediate results, since they would be deleted later.

### Reduce Input

- Reduce phase cannot achieve full data locality
- It processes results from all Map tasks, which may be distributed across the cluster

## Advantages of MapReduce

1. **Scalability**: Can process vast amounts of data by adding more nodes to the cluster
2. **Fault Tolerance**: Automatically handles failures and restarts tasks
3. **Simplicity**: Abstracts complex distributed computing details from developers
4. **Flexibility**: Can be used for a wide variety of data processing tasks

## Limitations

1. **Iterative Processing**: Not efficient for algorithms requiring multiple passes over the data
2. **Real-time Processing**: Batch-oriented nature makes it unsuitable for real-time analytics
3. **Complex Workflows**: Can be cumbersome for multi-stage or complex data pipelines

## Evolution and Modern Alternatives

While MapReduce was groundbreaking, newer technologies address some of its limitations:

- **Apache Spark**: Offers in-memory processing and a more flexible programming model
- **Apache Tez**: Offers a more flexible and efficient execution engine for Hadoop
